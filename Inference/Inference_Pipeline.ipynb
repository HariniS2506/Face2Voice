{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VCaQarRu8P-_"
      },
      "outputs": [],
      "source": [
        "!pip install facenet_pytorch"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "import torch\n",
        "import argparse\n",
        "from PIL import Image\n",
        "from facenet_pytorch import MTCNN, InceptionResnetV1\n",
        "from torchvision import transforms\n",
        "from facenet_pytorch.models.utils.detect_face import extract_face\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.autograd import Function"
      ],
      "metadata": {
        "id": "JsZLXj1jHmpA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from tensorflow.keras.preprocessing.image import load_img \n",
        "from tensorflow.keras.preprocessing.image import img_to_array \n",
        "from tensorflow.keras.applications.vgg16 import preprocess_input \n",
        "\n",
        "# models \n",
        "from keras.applications.vgg16 import VGG16 \n",
        "from keras.models import Model\n",
        "\n",
        "# clustering and dimension reduction\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "# for everything else\n",
        "import os\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from random import randint\n",
        "import pandas as pd\n",
        "import pickle"
      ],
      "metadata": {
        "id": "mI4aGTB_Hmsj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Whitening(object):\n",
        "    \"\"\"\n",
        "    Whitens the image.\n",
        "    \"\"\"\n",
        "    def __call__(self, img):\n",
        "        mean = img.mean()\n",
        "        std = img.std()\n",
        "        std_adj = std.clamp(min=1.0 / (float(img.numel()) ** 0.5))\n",
        "        y = (img - mean) / std_adj\n",
        "        return y"
      ],
      "metadata": {
        "id": "iBKxPLaWHq2P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class FaceFeaturesExtractor:\n",
        "    def __init__(self):\n",
        "        self.aligner = MTCNN(keep_all=True, thresholds=[0.6, 0.7, 0.9])\n",
        "        self.facenet_preprocess = transforms.Compose([Whitening()])\n",
        "        self.facenet = InceptionResnetV1(pretrained='vggface2').eval()\n",
        "\n",
        "    def extract_features(self, img):\n",
        "        bbs,_, landmarks = self.aligner.detect(img,landmarks=True)\n",
        "        if bbs is None:\n",
        "            # if no face is detected\n",
        "            return (None, None, None)\n",
        "        \n",
        "        faces = torch.stack([extract_face(img, bb) for bb in bbs])\n",
        "        embeddings = self.facenet(self.facenet_preprocess(faces)).detach().numpy()\n",
        "\n",
        "        return (bbs, embeddings, landmarks)\n"
      ],
      "metadata": {
        "id": "y87BnHVsHsM_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = VGG16()\n",
        "model = Model(inputs = model.inputs, outputs = model.layers[-2].output)\n",
        "\n",
        "def extract_features(file, model):\n",
        "    # load the image as a 224x224 array\n",
        "    #img = load_img(file, target_size=(224,224))\n",
        "\n",
        "    img = edge_detection(file)\n",
        "    (thresh, im_bw) = cv2.threshold(img, 128, 255, cv2.THRESH_BINARY | cv2.THRESH_OTSU)\n",
        "    img = cv2.cvtColor(im_bw,cv2.COLOR_GRAY2RGB)\n",
        "    # convert from 'PIL.Image.Image' to numpy array\n",
        "    img = np.array(img) \n",
        "    # reshape the data for the model reshape(num_of_samples, dim 1, dim 2, channels)\n",
        "    reshaped_img = img.reshape(1,224,224,3) \n",
        "    # prepare image for model\n",
        "    imgx = preprocess_input(reshaped_img)\n",
        "\n",
        "    # get the feature vector\n",
        "    features = model.predict(imgx, use_multiprocessing=True)\n",
        "    return features\n",
        "\n",
        "def crop_image(image):\n",
        "    features_extractor = FaceFeaturesExtractor()\n",
        "    bbs, embeddings, landmarks = features_extractor.extract_features(image)\n",
        "    image_array = np.array(image, dtype=np.float32)\n",
        "    image_array = cv2.cvtColor(image_array, cv2.COLOR_RGB2BGR)\n",
        "    bbs[:,1]-=5\n",
        "    bbs[:,3]+=15\n",
        "    crop_img = image_array[int(bbs[:,1][0]):int(bbs[:,3][0]), int(bbs[:,0][0]):int(bbs[:,2][0])]\n",
        "    return crop_img\n",
        "\n",
        "def edge_detection(image):\n",
        "  scale = 1\n",
        "  delta = 0\n",
        "  ddepth = cv2.CV_16S\n",
        "  src = cv2.imread(image, cv2.IMREAD_COLOR)\n",
        "  src = crop_image(src)\n",
        "  src = cv2.resize(src, (224,224), interpolation = cv2.INTER_AREA)\n",
        "  src = cv2.GaussianBlur(src, (3, 3), 0)\n",
        "  gray = cv2.cvtColor(src, cv2.COLOR_BGR2GRAY)\n",
        "  grad_x = cv2.Sobel(gray, ddepth, 1, 0, ksize=3, scale=scale, delta=delta, borderType=cv2.BORDER_DEFAULT)\n",
        "  grad_y = cv2.Sobel(gray, ddepth, 0, 1, ksize=3, scale=scale, delta=delta, borderType=cv2.BORDER_DEFAULT)\n",
        "    \n",
        "    \n",
        "  abs_grad_x = cv2.convertScaleAbs(grad_x)\n",
        "  abs_grad_y = cv2.convertScaleAbs(grad_y)\n",
        "    \n",
        "    \n",
        "  grad = cv2.addWeighted(abs_grad_x, 0.5, abs_grad_y, 0.5, 0)\n",
        "  return grad\n"
      ],
      "metadata": {
        "id": "TuHoZD9FH3P2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_image = 'Testing/test.png'\n",
        "feat = extract_features(test_image,model)"
      ],
      "metadata": {
        "id": "RilHvUq7H3Rs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "import os\n",
        "import numpy as np\n",
        "\n",
        "path = \"/content/Final_Clusters/\"\n",
        "faces = {}\n",
        "\n",
        "os.chdir(path)\n",
        "with os.scandir(path) as files:\n",
        "  for file in files:\n",
        "    cluster_num = file.name\n",
        "    subfolder_path = os.path.join(path, file)\n",
        "    with os.scandir(subfolder_path) as sub_files:\n",
        "      for sub_file in sub_files:\n",
        "        if '.ipynb_checkpoints' not in sub_file.name:\n",
        "          file_path = os.path.join(subfolder_path, sub_file)\n",
        "          file_temp = np.load(file_path)\n",
        "          faces[cluster_num] = file_temp"
      ],
      "metadata": {
        "id": "yTgnE3TYH3VJ"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "min_cluster = 0\n",
        "min_dist = float(\"inf\")\n",
        "\n",
        "for face in faces:\n",
        "  point1 = feat\n",
        "  point2 = faces[face]\n",
        "  dist = np.linalg.norm(point1 - point2)\n",
        "  if dist<min_dist:\n",
        "    min_cluster = face"
      ],
      "metadata": {
        "id": "Qrh-uOAUVwat"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_path = \"/content/Final_Clusters/\"+str(min_cluster)+\"/best_model.pth\"\n",
        "config_path = \"/content/Final_Clusters/\"+str(min_cluster)+\"/config.jscon\""
      ],
      "metadata": {
        "id": "Ue4zM5NZY2ZF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title\n",
        "%cd /content\n",
        "!sudo apt-get install espeak-ng\n",
        "!git clone https://github.com/coqui-ai/TTS.git\n",
        "!pip install TTS\n",
        "!tts --list_models"
      ],
      "metadata": {
        "id": "2t4PB7-FaK3S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from TTS.utils.manage import ModelManager\n",
        "from TTS.utils.synthesizer import Synthesizer\n",
        "use_cuda=False\n",
        "config_path = None\n",
        "speakers_file_path = None\n",
        "language_ids_file_path = None\n",
        "vocoder_path = None\n",
        "vocoder_config_path = None\n",
        "encoder_path = None\n",
        "encoder_config_path = None\n",
        "synthesizer = Synthesizer(\n",
        "        model_path,\n",
        "        config_path,\n",
        "        speakers_file_path,\n",
        "        language_ids_file_path,\n",
        "        vocoder_path,\n",
        "        vocoder_config_path,\n",
        "        encoder_path,\n",
        "        encoder_config_path,\n",
        "        use_cuda,\n",
        "    )\n",
        "text = \"I'm sorry, but I don't want to be an emperor.\"\n",
        "wav = synthesizer.tts(\n",
        "        text\n",
        "    )\n",
        "synthesizer.save_wav(wav, \"/content/test.wav\")"
      ],
      "metadata": {
        "id": "WYM9Rr4gWa5j"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}